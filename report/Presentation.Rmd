---
title: "An Introduction to the No U Turn Sampler and dual averaging"
author: "Henri Funk"
date: "3 12 2020"
output:
  revealjs::revealjs_presentation:
    self_contained: false
    theme: simple
    reveal_plugins: ["zoom", "menu", "notes"]
    center: false
    css: styles.css
    highlight: "zenburn"
    transition: "none"
    fig_caption: yes
    reveal_options:
      slideNumber: true
    autosize: true
header-includes:
  - \usepackage{diffcoeff,amssymb}
bibliography: references.bib
vertical-center : true
---
# {data-background=#262626}

<h1 style="color: #fff">Metropolis(-Hastings) Algorithm</h1>

<style>
p.caption {
  font-size: 0.6em;
  font-style: italic;
  color: grey;
  margin-right: 10%;
  margin-left: 10%;
  text-align: justify;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
```

<center>
```{r, echo=FALSE}
knitr::include_graphics("files/MH.png")
```
</center>

<!--
## Random Walk

> * proposal in MH is sampled randomly from porpoals/jumping distrib $J_t(\theta_a|\theta_b)$

> * the accpetance rule/ transition function ratio of densities guarantees convergence

<div class="fragment">
$$
\begin{aligned}
T_t(\theta^t|\theta^{t-1}) = \frac{\frac{p(\theta^*|y)}{J(\theta^*|\theta^{t-1})}}{\frac{p(\theta^{t-1}|y)}{J(\theta^{t-1}|\theta^*)}}
\end{aligned}
$$
</div>

> * *But* if steps are poorly chosen converges speed, and accordingly, computational effort can get high

<div class="fragment">
$J(\cdot)$ is only useful jumping distibution, if:

* For any $\theta$ it is easy to sample from $J(\theta_a|\theta_b)$

* It is easy to compute r (e.g. if proposal is symmetric)

* The jump has a *reasonable* distance

* We don't reject/ accept jumps to often
 (see @gelman13)
</div>
-->

## Random Walk Problem

```{r, echo=FALSE, out.width='75%'}
knitr::include_graphics("files/HMC_multimodal.gif")
```
@feng20

> * Gibbs and MH spend a lot of time zigging and zagging in the target distribution

> * Convergence speed depends on the Jumping distribution



# {data-background=#262626}

<h1 style="color: #fff">Hamiltonian Monte Carlo</h1>

<center>
```{r, echo=FALSE}
knitr::include_graphics("files/HMC.png")
```
</center>

## Idea
<br><br>

> * Introduce a momentum $\phi_j$ for each component $\theta_j$
> * update $\theta\ \& \ \phi$ simultaneously
> * Let the jumping distribution $J(\cdot)$ be largely determined by $\phi$

<div class="fragment">
$\Rightarrow$ the resulting algorithm is somewhat a *hybrid* Monte Carlo with a
mix of the known random walk and deterministic simulation methods derived from hamiltonian dynamics
</div>

<!---
## HMC in 3 steps:

> 1) update $\phi$, sampled from $\phi \sim \mathcal{N}(0, I)$    
> 2) simultaneously updating of $(\theta,\phi)$ via leapfrog steps    
> 3) acceptance/ rejection step analog to MH-Algorithm    

<div class="fragment">

### Ingredients

* (un-normalized) log posterior density $log(p(\theta|y))$    
</div>

<div class="fragment">
* gradient of $log_e(p(\theta|y))$: 
$\frac{\partial log_e(p(\theta|y))}{\partial \theta}$  
(Note: use analytic solutions here if possible, numerical might make the computational benefit vanish)
</div>

<div class="fragment">
* the momentum distribution $p(\phi)$  
$\Rightarrow$ usually $\phi \sim \mathcal{N_d}(0, I)$ where 0 denotes a zero vector of $\mathbb{R^d}$ and I denotes a $\mathbb{R^{dxd}}$ identity matrix
</div>


## Phylical analogy - Why use log?

<center>
```{r, echo=FALSE, out.width='50%'}
knitr::include_graphics("files/log_gradient.png")
```
</center>
@moore

<div class="fragment">
* Suppose HMC moves towards *low posterior density regions*:


$$
sgn\Big(\frac{dlog_e(p(\theta|y))}{d\theta}\Big) = -1 \ \Rightarrow \ \phi\ decreases
$$
</div>

<div class="fragment">

* Suppose HMC moves towards *a local posterior maximum*:

$$
\frac{dlog_e(p(\theta|y))}{d\theta} = 0 \ \Rightarrow \ p(\theta|y)\ slows \ by\ moving\ closer\ to\ local\ max
$$
</div>

<div class="fragment">
$\Rightarrow$ HMC works like a mode-finder
</div>

## Phylical analogy

<center>
```{r, echo=FALSE, out.width='75%'}
knitr::include_graphics("files/halfpipe.gif")
```
</center>

## Code - Parameter

```{r, code=readLines("../R/Hamiltonian_MonteCarlo.R")[c(1:9, 42)]}
```


## Code - initilaize Values

```{r, code=readLines("../R/Hamiltonian_MonteCarlo.R")[c(1:14, 42)]}
```

## Code - HMC iteration

```{r, code=readLines("../R/Hamiltonian_MonteCarlo.R")[c(1:21, 40, 42)]}
```

## Code - Leapfrogsteps

```{r, code=readLines("../R/Hamiltonian_MonteCarlo.R")[c(1:30, 40, 42)]}
```

-->

## Code - HMC

```{r, code=readLines("../R/Hamiltonian_MonteCarlo.R")}
```

## {data-background="files/HMC_problem.gif"} 
Problem
@feng20

> * taking too few or too large steps $\Rightarrow \ \theta^t\ \& \ \theta^{t+1}$ end up very close. 

## (Hand-)Tuning Parameters

* $\epsilon = stepsize$


* $\mathcal{L} = \#Leapfrogsteps$


* $\phi = momentum$


<div class="fragment">
But how?

<center>
```{r, echo=FALSE, out.width='40%'}
knitr::include_graphics("files/Questioning.gif")
```
</center>
</div>

## Setting the tuning parameters

<div class="fragment">
* *Radius*:
  
  stay in the radius of you target distribution. Rule of thumb: $\epsilon \mathcal{L} = 1$
</div>

<div class="fragment">
* *adaptive updating*: 

  1) run $M_{init}$ steps with initial setting

  2) adjust the parameters based on knowledge from previous run and rerun the model for $M_{adjust}$ steps
</div>

<div class="fragment">
* *Acceptance Sweet Spot*: 65% acceptance rate ($\alpha$)
  
  if $\alpha < 0.65 \Rightarrow$ leapfrog jumps are too ambitious.
  Set $\mathcal{L}\uparrow, \epsilon \downarrow$.
  
  if $\alpha > 0.65 \Rightarrow$ leapfrog jumps are too cautious.
  Set $\mathcal{L}  \downarrow, \epsilon \uparrow$.
(see @gelman13)
</div>

## Desirable behavoir for tuning parameters tuning parameters HMC:

<br>

> 1) $\mathcal{L}$ driving the trajectory of steps in one iteration though the whole posterior space.  
*Tackeled by NUTS*

<br>

> 2) $\epsilon$ getting smaller in areas of high curvature exploiting various areas.  
*Tackeled by dual averaging*

<br>

> 3) $Covar(\phi)$ scaling to the local curvature.  
*Riemanian Integral*

<br>

<div class="fragment">
$\Rightarrow$ All three approaches may be combined. In the following we will discuss the implementation for a No-U-Turn sampler with dual averaging, as discussed in Gelman and Hoffman (2014).
@hoffman2014
</div>

# {data-background=#262626}

<h1 style="color: #fff" class="r-fit-text">The (naive) No-U-Turn Sampler</h1>

<center>
```{r, echo=FALSE}
knitr::include_graphics("files/nNUTS.png")
```
</center>

<!--
NOTES:
> * As we use the information from Hamiltonian dynamics to tune $\epsilon$ and $\mathcal{L}$ within the run iteration don't guarantee convergence anymore

> * Property of markov chain is not given anymore

<div class="fragment">
* What do we do now?

  + Derive a heuristic when we want to stop the sampling
  + Ensure the adaption satisfies the Markov Chain properties
</div>
-->

## Heuristic Stopping Criteria

</small>
<div class="fragment">

* Lets look at one iteration $t$ in HMC where we intentionally set
  a) $\mathcal{L_t}$ to small
  b) $\mathcal{L_t}$ to high
</div>

<div class="fragment">
* The $\mathcal{L_t}$-Leapfrog steps performed from starting state $(\theta_{right}, r_{right})$ in phase-space to their end  $(\theta_{left}, r_{left})$ after $\mathcal{L_t}$ Leapfrogsteps might look as follows:
</div>

```{r, echo=FALSE}
lable1 <- "Left Plot: Expanding the trajectory in either direction typically extends the trajectory further  across the energy level set (grey) towards unexplored neighborhood. Right Plot: Further expansion typically contracts the boundaries of the trajectory towards each other and neighborhoods that have already been explored."
```

<div class="fragment">
<center>
```{r, echo=FALSE, fig.cap=lable1, out.width="50%"}
knitr::include_graphics("files/U-Turn-Criteria.png")
```
</center>
</div>
</small>

## Criterion

$\Rightarrow$ explore the energy level stepwise @betancourt2017

$$
r_{left} (\theta_{right} - \theta_{left}) < 0\\
r_{right} (\theta_{left} - \theta_{right}) < 0
$$

<div class="fragment">
*Derivation:* this is basically a euclidean derivative w.r.t. time t of half the squared distance between two positions: $\frac{d(\theta_1 - \theta_2)^T(\theta_1 - \theta_2)}{dt}$.

*Intuition:* this is the dot product of the two vectors from the picture.
If the dot product gets higher than 0, the trajectory will make a *u-turn*.
This is when we "traveled maximally" in phase space an so where we want to stop exploring the enery set.
</div>

## Naive Stepwise Exploration Scheme

<br>

> 1) Build a trajectory with a given length

<br>

> 2) Check the Termination (No-U-Turn) Criterion

<br>

> 3) Expand the trajectory and & Repeat Checks

<br>

> 4) Return the sample once the criterion is met

<br>

<div class="fragment">
**Naive Additive Scheme**: Check the termination Criteria for each and every point between any two points
Smart
<br>
**Multiplicative Scheme**: Double the trajectory to the old trajectory and so create a balanced binary tree. Compare the criterion only between subtrees.
</div>

## Doubling- expand the trajectory

```{r, echo=FALSE}
lable2 <- "Typical doubling proccedure. The initial point is black. Each colour desricbes a new subtree."
```

<center>
```{r, echo=FALSE, fig.cap=lable2, out.width="75%"}
knitr::include_graphics("files/Doubling.png")
```
@hoffman2014  
</center>

<div class="fragment">
The doubling is halted when the sub-trajectories from the leftmost to the rightmost nodes of any balanced binary tree start to double back on themselves.

*Info:* We have to double fore- and backwards in our trajectory to guarantee reversibility in time
</div>

## Slice to the Rescue - Valid States

<div class="fragment">
Some of the trajectory points sampled have to be condemn because they exhibit pathological behavior.
<center>
```{r echo=FALSE, out.width="50%"}
knitr::include_graphics("files/Leapfrog_Discretization.png")
```
<center>

<div class="fragment">
To determine those points we introduce a slice variable $u$ to our posterior
</div>

<div class="fragment">
We sample from $u \sim  Unif(u; [0, e^{\mathcal{L}(\theta_t) - \frac{1}{2} <r_t,r_t>}])$ and for each state $(\hat{\theta}, \hat{r})$ we propose during the next iteration $t+1$ the unnormalized joint posterior at this point has to be higher than u
to be **valid**.
</div>

<div class="fragment">
$(\hat{\theta}, \hat{r})$ is only a valid state if:

$$
u \leq exp(\mathcal{L}(\hat{\theta}) - \frac{1}{2} <\hat{r},\hat{r}>)
$$
</div>

<div class="fragment">
This is comparable to the acceptance / rejection step in HMC
</div>

<div class="fragment">
*Alternative:* draw from multinomial over states in the trajectory 
</div>


## $\mathcal{B}, \mathcal{C}$ in NUTS

$\mathcal{B}$: constituted by all leaves of the binary tree generated in the doubling proccedure of one NUTS iteration.
If $j$ denotes the depth of a tree the amount of visited states through doubling in leapfrog trajectory is $\#\mathcal{B} = 2^j$.  
$\mathcal{C}$: contains all valid states visited.

<div class="fragment">
```{r, echo=FALSE}
lable3 <- "Example of a trajectory generated during one iteration of NUTS."
```

<center>
```{r, echo=FALSE, fig.cap=lable3, out.width="50%"}
knitr::include_graphics("files/trajectory.png")
```
</center>
</div>

## Error correction Stopping Criteria

Lets sample a slice using information from the position and momentum sampled in the previous iteration.

The slice helps us to build another stopping criteria.
It berries information of the likelihood from our previous iteration, as it is sampled from $u \sim  Unif(u; [0, e^{\mathcal{L}(\theta_t) - \frac{1}{2} <r_t,r_t>}])$.
So we can use it to identify errors from our hamiltonian trajectory:

Given a new proposal state $(\hat{\theta}, \hat{r})$ sampled via leapfrogsteps, we stop our doubling, if

$$
\begin{aligned}
\mathcal{L}(\hat{\theta}, \hat{r}) - log(u)& < -\Delta_{max} \\
 = log(\frac{p(\hat{\theta}, \hat{r})}{u})& < -\Delta_{max}
\end{aligned}
$$
Set $\Delta_{max}$ to a very high value like 1000.
So basically if $p(\hat{\theta}, \hat{r})$ is the proportional likelihood is way less likely compared to our slice we stop simulating.

## Implement NUTS - Initialize

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_naive_NUTS.R")[c(1:15, 76)]}
```

## Implement NUTS - Itertation

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_naive_NUTS.R")[c(1:31, 74, 76)]}
```

## Implement NUTS - Doubling the tree

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_naive_NUTS.R")[c(1:63, 74, 76)]}
```

## Implement NUTS - Sample a state

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_naive_NUTS.R")}
```

## Implement Build Trees - Initialize

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Build_Tree.R")[c(1:20, 68)]}
```

## Implement Build Trees - Base Case

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Build_Tree.R")[c(1:28, 51, 68)]}
```

## Implement Build Leaf

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Build_Tree.R")[70:104]}
```

## Implement Build Trees - Doubled Recurion

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Build_Tree.R")[c(1:51, 67, 68)]}
```

## Implement Build Trees - Update State

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Build_Tree.R")[c(1:68)]}
```


# {data-background=#262626}

<h1 style="color: #fff" class="r-fit-text">No-U-Turn Sampler with dual averaging</h1>

<center>
```{r, echo=FALSE}
knitr::include_graphics("files/NUTS.png")
```
</center>

## Problems of naive NUTS

Recall the example trajectory:


<div class="fragment">
```{r, echo=FALSE}
lable3 <- "Example of a trajectory generated during one iteration of NUTS."
```

<center>
```{r, echo=FALSE, fig.cap=lable3, out.width="50%"}
knitr::include_graphics("files/trajectory.png")
```
</center>
</div>


> * Problem 1: waste of posterior and gradient computations 

> * Problem 2: large amount of memory: Storing $2^j$ position-momentum states

> * Problem 3: if stopping is met in the middle of a run there is no need for further evaluations


## Efficient NUTS - a solution

Idea:

* Implement a transition kernel that produces larger jumps than uniform sampling while keeping $\mathcal{C}$ balanced.

* Therefore sample from $\mathcal{C}$ incrementally by its subtrees

* Quit early if any stopping criteria is met

<div class="fragment">
For each subtree $\mathcal{B_{st}}$ in $\mathcal{B}$:

> * sample a valid state $(\theta, r) \in \mathcal{C_{st}}$ as representee

> * choose a pair by giving them a weight proportional to their subtrees element size

</div>

<div class="fragment">
* $\Rightarrow$ this method stores only $\mathcal{O}(j)$ position momentum vectors and

* is sparse at calculating the gradient as build tree can be canceled early if U-turn was made
</div>

## Adaptively tuning the stepsize

<div class="fragment">
Idea: 
Find a **stepsize** $\epsilon$ in a Warm-Up Phase that guarantees quick convergence
Set this value in a Stationary Phase to make the algorithm converge 
</div>

<div class="fragment">
*Suppose* we aim for a target average acceptance probability $\delta (= 0.65)$.

*Suppose* our average acceptance at iteration t is $\alpha_t$

Let $H_t = \delta - \alpha_t$ then be our MCMC behavior at iteration t

*Goal:* reach $H_t \approx 0$
</div>

<div class="fragment">
*Update* $\epsilon$ as follows:

$\epsilon_{t+1} =\epsilon_t - \eta_tH_t, \ \eta_t \in (0,1]$
</div>

<div class="fragment">
If acceptance $\alpha_t$ was to high we encourage the algorithm for larger jumps, rising $\epsilon_{t+1}$

If acceptance $\alpha_t$ was to low we encourage the algorithm for smaller jumps, decreasing $\epsilon_{t+1}$
</div>

## Intuition 

> * Acceptance probability for this, would be one if we could exactly run the hamiltonian dynamics as it drives us towards lower density regions.

> * $\Rightarrow$ Essentially, this acceptance probability is related to how good the numerical approximation of Leapfrogsteps to the Hamiltonian dynamics is. 

> * Then, we balance the tradeoff between the error and the time it takes to generate any given sample by varying $\delta$ either spend more time generating better approximations of the Hamiltonian-dynamics, or alternately spend less time generating crappier ones knowing that we will throw more away.

<div class="fragment">
<center>
```{r echo=FALSE, out.width="75%"}
knitr::include_graphics("files/Leapfrog_Discretization.png")
```
<center>
</div>

## Dual Averaging

**Problem:** Usually parameters are quite different between Warm-Up and Stationary phase.

$\Rightarrow$ We want the stepsize to adapt quickly as we shift from Warm-Up to stationary Phase

<div class="fragment">
$$
H^t = (1 - \frac{1}{t_0 + t}) H^{t-1} + \frac{1}{t_0 + t} (\delta - \frac{\alpha}{n_{\alpha}})\\
log(\epsilon^t) = \mu - \frac{\sqrt{t}}{\gamma} H^t \\
log(\overline{\epsilon}^t) =t^{-\kappa} \epsilon^t + (1-t^{-\kappa}) \overline{\epsilon}^{t-1}
$$
</div>

> * $t$ is the iteration we are at

> * $t_0$ stabilizes $H^t$ in early iterations (default: 10L)

> * $\epsilon^t$ epsilon at itaeration t (this is the next iteration here)

> * $\mu$ freely choosen value where we shrink towards (default: $log(10\epsilon^1)$, which encourages the algorithm for larger $\epsilon$)

> * $\gamma$ controls the amount of shrinkage (default: 0.05)

> * $t^{-\kappa}$ stepsize schedual: increases the influence of more recent iterations

<div class="fragment">
We use $\epsilon^t$ in Warm-Up phase for $M_{adapt}$ iterations, and $\overline{\epsilon}^{M_{adapt}}$ for the stationary phase.

$\epsilon^t$ allows the algorithm in Warm Up Phase to adapt quickly

$\overline{\epsilon}^{M_{adapt}}$ ensures a stable stepsize in stationary phase that favors more recent iterations from Warm Up phase
</div>

## Dual Averaging in NUTS

Problem: As NUTS has no clarified acceptance/rejection step we need to specify this step on our own

So we have to recalculate acceptance:

<div class="fragment">
$$
\begin{aligned}
\alpha_t^{NUTS} & = \frac{1}{|\mathcal{B_t^{final}}|}\sum_{(\theta, r) \in \mathcal{B_t^{final}}} min(1, \frac{p(\theta, r)}{p(\theta^{t-1}, r^{t-1})})
\end{aligned}
$$
</div>
<div class="fragment">

Integration in `build_leaf`:

```{r, code=readLines("../R/Build_Tree.R")[95:120]}
```

</div>


## Find initial stepsize

> * Dual averaging works for any given starting value $\epsilon_1$

> * However we can speed up convergence by choosing a propper $\epsilon_1$

> * We constantly double/halve proposal $\epsilon_1$ until the acceptance rate of our proposal crosses 0.5 or 2

>   + For low exceptance we encourage the algorithm for small cautious steps

>   + For high exceptance we encourage the algorithm for large ambitious steps

<div class="fragment">
```{r, warning=FALSE, code=readLines("../R/Find_initial_stepsize.R")}
```
</div>

## Intuition behind automatising the step size:

With that said, the idea is that the slice sampler is there because we can't solve the dynamical system exactly, but with a discrete system. Because it's going to have an error, we use the slice sampler to compensate for bias that might exist if we just accepted the numerical Hamiltonian dynamics as if they were exact.



## Implement efficient NUTS with dual averaging

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Efficient_NUTS.R")}
```

## Build efficient tree and leaves

```{r, warning=FALSE, code=readLines("../R/Showcase_Functions/Simplified_Double_Efficient.R")}
```

# Examples

<center>
```{r, echo=FALSE}
knitr::include_graphics("files/DancingPeaks.gif")
```
</center>

## Sample from $\mathcal{N_{50}}(0, I)$

```{r, include=FALSE, warning=FALSE, message=FALSE}
rm(list = c("acceptance_rate", "build_leaf", "find_initial_stepsize", "hamiltonianMC", "sample_noUturn"))
longrun = FALSE

devtools::load_all()
library(ggplot2)
library(gridExtra)
```

The call:

```{r, eval=longrun}
posterior_density = mvtnorm::dmvnorm
gradient <- function(position){-position}
sample50norm <- sample_noUturn(position_init = runif(50, -5, -2), iteration = 2e3, is_log = FALSE)
```

```{r, echo=FALSE, eval=longrun}
saveRDS(sample50norm, file = "files/sample50norm.rds")
```

```{r, echo=FALSE, eval =!longrun}
sample50norm <- readRDS("files/sample50norm.rds")
```

<div class="fragment">
Trace the algorithm:

```{r echo=FALSE, fig.height=3}
a <- ggplot(data = sample50norm, aes(x = seq_len(2000), y = X10)) +
  geom_line(colour = "blue")
b <- ggplot(data = sample50norm[-seq_len(1e3), ], aes(x = X10)) +
  geom_density(colour = "blue")
c <- ggplot(data = sample50norm, aes(x = seq_len(2000), y = X40)) +
  geom_line(colour = "green")
d <- ggplot(data = sample50norm[-seq_len(1e3), ], aes(x = X40)) +
  geom_density(colour = "green")
grid.arrange(a, b, c, d, nrow = 1)
```

</div>

## Regression - an artificial data set

Generate data: 

```{r}
design <- cbind(1, sapply(1:2, function(x) runif(400)))
perfect_position <- c(1,2,3)
target <- design %*% perfect_position + rnorm(400)
```

Take a look at the data:

```{r, echo=FALSE}
ggplot(as.data.frame(design[,-1]), aes(x =V1, y = V2, colour=target)) +
  geom_point() +
  scale_colour_gradient2()
```

## Regression - Comparison

```{r}
lm(target~., data = as.data.frame(design[,-1]))
```

## Regression - Set Up NUTS

```{r}
posterior_density = log_linear
gradient = partial_deriv_lin
```

```{r, eval=longrun}
linear <- sample_noUturn(c(4, 4, 4), iteration = 2e3, design = design, target = target,
                         is_log = TRUE, seed = 123L, target_accpentance = .65, max_tree_depth = 12L)
```

```{r eval=longrun, echo=FALSE}
saveRDS(object = linear, file = "files/linear.RDS")
```

```{r eval=!longrun, echo=FALSE}
linear <- readRDS("files/linear.RDS")
```

## Regression - Trace Plot for parameters

```{r, echo=FALSE}
calculate_traceplots <- function(var, varname, iter = 2e3){
  ggplot(data = linear, aes_string(x = seq_len(iter), y = var)) +
    geom_line(color = "red") + labs(x = "iteration", y = varname) +
    annotate("rect", xmin = 0, xmax = 1e3, ymin = 4, ymax = -1, alpha = .2)
}

args <- list(
  list(var = "X1", varname = "intercept"), list(var = "X2", varname = "slope2"),
  list(var = "X3", varname = "slope3"))

traceplots <- lapply(args, function(x) do.call(calculate_traceplots, x))

do.call(grid.arrange, c(traceplots, nrow = 2))
```

## Classification - Endometrial data Set

* **Goal:** Predict whether a person has endometrial cancer or not.

<div class="fragment">
**Dataset:** 
>A data frame with 79 rows and 4 variables:
NV Neovasculation risk factor indicator (0=Absent, 1=Present)  
PI Pulsatility index of arteria uterina  
EH Endometrium height  

</div>

<div class="fragment">
* We standardize the data to get an estimates independent from their scale.
</div>

```{r, echo=FALSE}
library(hmclearn)
data(Endometrial)


# data prep
Endometrial$PI2 <- with(Endometrial, (PI - mean(PI)) / sd(PI))
Endometrial$EH2 <- with(Endometrial, (EH - mean(EH)) / sd(EH))
Endometrial$NV2 <- Endometrial$NV - 0.5

X <- cbind(1, as.matrix(Endometrial[, which(colnames(Endometrial)
            %in% c("PI2", "EH2", "NV2"))]))
y <- Endometrial$HG

colnames(X) <- c("(Intercept)", "PI2", "EH2", "NV2")
```

## Classification - Define posterior and gradient function

<small>
<div class="fragment">
$$ \begin{aligned}
f(\mathbf{y} | \mathbf{X},\boldsymbol\beta) &= \prod_{i=1}^n p^{y_i} (1-p)^{1-y_i}, \\
&= \prod_{i=1}^{n} \left(\frac{1}{1+e^{-\mathbf{x}i^T\boldsymbol\beta}}\right)^{y_i} \left(\frac{e^{-\mathbf{x}_i^T\boldsymbol\beta}}{1+e^{-\mathbf{x}_i^T\boldsymbol\beta}}\right)^{1-y_i}, \\
\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &= \sum{i=1}^n -y_i\log(1+e^{-\mathbf{x}i^T\boldsymbol\beta}) + (1-y_i)(-\mathbf{x}_i\boldsymbol\beta - \log(1+e^{-\mathbf{x}_i^T\boldsymbol\beta})), \\
&= \sum{i=1}^n -\log(1+e^{-\mathbf{x}i\boldsymbol\beta}) - \mathbf{x}_i^T\boldsymbol\beta(1 - y_i), \\
&= \sum{i=1}^n \mathbf{x}_i^T\boldsymbol\beta(y_i - 1) - \log(1 + e^{-\mathbf{x}_i^T\boldsymbol\beta}), \\
&= \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}_n) - \mathbf{1}_n^T [ \log( 1 + e^{-\mathbf{X}\boldsymbol\beta})].
\end{aligned} $$

</div>
<div class="fragment">
We set a multivariate normal prior for $\boldsymbol\beta$

$$
\begin{aligned} \boldsymbol\beta &\sim N(0, \sigma^2 \mathbf{I}), \end{aligned}
$$

</div>
<div class="fragment">
with pdf, omitting constants

$$
\begin{aligned}
\pi(\boldsymbol\beta | \sigma^2) &= \frac{1}{\sqrt{\lvert 2\pi \sigma^2 \rvert }}e^{-\frac{1}{2}\boldsymbol\beta^T \boldsymbol\beta / \sigma^2}, \\
\log \pi(\boldsymbol\beta | \sigma^2) &= -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2}\boldsymbol\beta^T \boldsymbol\beta / \sigma^2, \\
&\propto -\frac{1}{2}\log \sigma^2 - \frac{\boldsymbol\beta^T\boldsymbol\beta}{2\sigma^2}. 
\end{aligned}
$$
</div>
</small>

## Classification - Define posterior and gradient function II
<small>
<div class="fragment">
Next, we derive the log posterior, omitting constants,

$$
\begin{aligned}
f(\boldsymbol\beta | \mathbf{X}, \mathbf{y}, \sigma^2) &\propto f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) \pi(\boldsymbol\beta | \sigma^2), \\
\log f(\boldsymbol\beta | \mathbf{X}, \mathbf{y}, \sigma^2) &  \propto \log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) + \log \pi(\boldsymbol\beta|\sigma^2), \\
&\propto \sum_{i=1}^n \mathbf{x}i^T\boldsymbol\beta(y_i - 1) - \log(1 + e^{-\mathbf{x}_i^T\boldsymbol\beta}) - \frac{1}{2}\boldsymbol\beta^T\boldsymbol\beta / \sigma^2, \\
&\propto \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}n) - \mathbf{1}_n^T[\log( 1 + e^{-\mathbf{X}\boldsymbol\beta})] - \frac{\boldsymbol\beta^T\boldsymbol\beta}{2\sigma^2}. 
\end{aligned} 
$$
</div>
<div class="fragment">
Next, we need to derive the gradient of the log posterior for the leapfrog function

$$ 
\begin{aligned}
\nabla_{\boldsymbol\beta} \log f(\boldsymbol\beta, \mathbf{X}, \mathbf{y}, \sigma^2) &\propto \mathbf{X}^T \left ( \mathbf{y} - \mathbf{1}n+ \frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right) - \boldsymbol\beta / \sigma^2 
\end{aligned}
$$
</div>
</small>
<div class="fragment">
Finally simply initialize the functions:

```{r}
posterior_density = bernoulli_pen
gradient = partial_deriv_sigmoid
```
</div>

## Classification - Run NUTS

Run `sample_noUturn` for 5000 iterations with a small target acceptance of `0.39`
As our posterior is already logged we ca specify the `is_logged` argument as `TRUE`

We use a weak prior where $\sigma^2I = 200I$, this is analog t a very week penalty.

<div class="fragment">
```{r, eval=longrun}
initial_pos <- lapply(1:5, function(x) runif(4, -5, 5))
param_sample_binary <- lapply(initial_pos, sample_noUturn, iteration = 5e3,
                              design = X, target = y,
                              is_log = TRUE, target_accpentance = .39,
                              max_tree_depth = 12L)
```

```{r eval=longrun, echo=FALSE}
saveRDS(object = param_sample_binary, file = "files/param_sample_binary.RDS")
```

```{r eval=!longrun, echo=FALSE}
param_sample_binary <- readRDS("files/param_sample_binary.RDS")
```
</div>

This took 24 hours of calculation, plus preparation.

## Trace the run

```{r}
attrib <- lapply(param_sample_binary, attributes)
stepsize <-as.data.frame(sapply(attrib, function(x) log(x$dual_averaging$stepsize)))
ggplot(stepsize, aes(x = seq_len(2501), y = V1)) +
  geom_line(colour = "red") +
  geom_line(aes(y = V2), colour = "blue") + ylab("log stepsize value during Warm Up") + xlab("iteration")
  
```

Sationary epsilon after adaptive Warm Up:

```{r, echo=FALSE}
attrib <- lapply(param_sample_binary, attributes)
stat_eps <- sapply(attrib, function(x) x$dual_averaging$stepsize_weight[2501])
```


## Classification - Check Convergence by Traceplots

```{r, echo=FALSE}
calculate_traceplots <- function(var, varname, iter = 5e3){
  ggplot(data = param_sample_binary[[1]], aes_string(x = seq_len(iter), y = var)) +
    geom_line(color = "red") + labs(x = "iteration", y = varname) +
    geom_line(data =param_sample_binary[[2]],
            aes_string(x = seq_len(iter), y = var), color = "blue") +
    geom_line(data =param_sample_binary[[3]],
            aes_string(x = seq_len(iter), y = var), color = "green") +
    geom_line(data =param_sample_binary[[4]],
            aes_string(x = seq_len(iter), y = var), color = "purple") +
    annotate("rect", xmin = 0, xmax = 2500, ymin = -5, ymax = 15, alpha = .2)
}

args <- list(
  list(var = "X1", varname = "intercept"), list(var = "X2", varname = "slope2"),
  list(var = "X3", varname = "slope3"), list(var = "X4", varname = "slope4")
  )

traceplots <- lapply(args, function(x) do.call(calculate_traceplots, x))

do.call(grid.arrange, c(traceplots, nrow = 2))
```

## Classification - Check Convergence by Density Plots

```{r, echo=FALSE}
param_distrib <- lapply(param_sample_binary, function(x){
  x[-seq_len(4e3),]
})

calc_density_plots <- function(var, varname) {
  ggplot() +
    geom_density(data = param_distrib[[1]], aes_string(x = var), color = "red") + 
    geom_density(data = param_distrib[[2]], aes_string(x = var), color = "blue") + 
    geom_density(data = param_distrib[[3]], aes_string(x = var), color = "green") +
    geom_density(data = param_distrib[[4]], aes_string(x = var), color = "purple") +
    ggtitle(paste("density", varname))
}

density <- lapply(args, function(x)calc_density_plots(x$var, x$varname))
do.call(grid.arrange, c(density, nrow = 2))
```

## Classification - Prediction and Comparison

<div class="fragment">
Lets compare our models mean parameter Estimators to those of a GLM

```{r}
f <- glmnet::glmnet(X, y, family = "binomial", lambda = 1/200)
mean_estimate <- lapply(param_distrib, colMeans)
```
</div>
<div class="fragment">

```{r, echo=FALSE}
reg_glm <- rbind(t(sapply(param_distrib, colMeans))[-5,], c(f$a0, f$beta@x))
reg_glm <- round(as.data.frame(reg_glm, row.names =  c("Sample1", "Sample2", "Sample3", "Sample4", "regularized GLM")), 2)
knitr::kable(reg_glm, col.names = c("intercept", "slope1", "slope2", "slope3"),
             caption =  "parameter Estimates averaged for last 1000 iterations vs penalized GLM")
```

</div>
<div class="fragment">

In the following we can see the average deviation of one prediction from the true value is about 20% uncertainty:

```{r, echo=FALSE}
calc <- lapply(mean_estimate, function(x) {
  1- 1/(1+ exp(X %*% x))
})

sapply(calc, function(x){
  sum(sqrt((y - x)^2)) / nrow(x)
})[1:4]
```
</div>
<!---
## Regression - Data Set

**Data Set Information:**

The data was retrieved from a set of 53500 CT images from 74 different
patients (43 male, 31 female).

<div class="fragment">

Each CT slice is described by two histograms in polar space.
The first histogram describes the location of bone structures in the image,
the second the location of air inclusions inside of the body.
Both histograms are concatenated to form the final feature vector.
Bins that are outside of the image are marked with the value -0.25.

</div>
<div class="fragment">

The class variable (relative location of an image on the axial axis) was
constructed by manually annotating up to 10 different distinct landmarks in
each CT Volume with known location. The location of slices in between
landmarks was interpolated.

</div>
<div class="fragment">

**Attribute Information:**

1. patientId: Each ID identifies a different patient
2. - 241.: Histogram describing bone structures
242. - 385.: Histogram describing air inclusions

*target:* 

reference: Relative location of the image on the axial axis (class
value). Values are in the range [0; 180] where 0 denotes
the top of the head and 180 the soles of the feet.


</div>


## Regression - Define posterior and gradient function

<div class="fragment">
$$ \begin{aligned}
f(\mathbf{y} | \mathbf{X},\boldsymbol\beta, v) &= \prod_{i=1}^n \frac{1}{\Gamma(v)}(\frac{vy_i}{\mu_i})^ve^{\frac{v}{\mu_i}y_i}\frac{1}{y_i}, \\
\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta, v) &= \sum_{i=1}^n (v-1) log(y_i) - \frac{v}{\mu_i}y_i+vlog(v)-vlog(\mu_i)-log(\Gamma(v)), \\
&= \sum_{i=1}^n (v-1) log(y_i) - \frac{v}{exp(x_i^T\beta)}y_i+vlog(v)-vx_i^t\beta-log(\Gamma(v)),\\
\Delta_{\beta}\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta, v)&\propto vX^T((\frac{1}{exp(X\beta)}\otimes y) - 1), \\
\Delta_{v}\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta, v)&\propto \sum_{i=1}^n log(y_i) -  \frac{1}{exp(x_i^T\beta)}y_i - log(v) -1 - x_i^t\beta- \frac{1}{\Gamma(v)} \Gamma^{\prime}(v)
\end{aligned} $$

<div class="fragment">
Finally simply initialize the functions:

```{r, eval=longrun, echo=FALSE}
id <- sample(seq_len(53500), 1e4)
trainCT <- ct_slice[id, ]
target <- trainCT[, 386]
design <- as.matrix(cbind(1, trainCT[, -386]))
```

```{r, eval=longrun}
posterior_density = gamma_distr
gradient = partial_deriv_gamma
```
</div>

## Regression - Run NUTS

```{r, eval=longrun}
start <- Sys.time()
initial_pos <- lapply(1:3, function(x) c(runif(1), runif(386, 16, 18)))
sigmoid_parameter_distrib <- lapply(initial_pos, sample_noUturn, iteration = 10, design = design, target = target, is_log = TRUE, seed = 123L, target_accpentance = .65, max_tree_depth = 11L)
dif2000iter <- Sys.time() - start
dif2000iter
```

--->

# {data-background="files/stan.png" data-background-size="800px 600px"}

## Presettings

**Loading the package** 

```{r, eval=FALSE}
library("rstan")
```

**To estimate your model in parallel**

```{r, eval=FALSE}
options(mc.cores = parallel::detectCores())
```

Automatically save a bare version of a compiled Stan program to the hard disk so that it does not need to be recompiled

```{r, eval=FALSE}
rstan_options(auto_write = TRUE)
```

You will need to run these commands each time you load the rstan library.

## Example Eight Schools - SetUp

This is an example in Section 5.5 of @gelman13, which studied coaching effects from eight schools.

We start by writing a Stan program for the model in a text file.
If you are using RStudio click on `File -> New File -> Stan File` . 
Either way, paste in the following and save your work to a file called `schools.stan` in R's working directory.

```{stan, output.var='priors', eval = FALSE, tidy = FALSE}
// saved as schools.stan
data {
  int<lower=0> J;         // number of schools 
  real y[J];              // estimated treatment effects
  real<lower=0> sigma[J]; // standard error of effect estimates 
}
parameters {
  real mu;                // population treatment effect
  real<lower=0> tau;      // standard deviation in treatment effects
  vector[J] eta;          // unscaled deviation from mu by school
}
transformed parameters {
  vector[J] theta = mu + tau * eta;        // school treatment effects
}
model {
  target += normal_lpdf(eta | 0, 1);       // prior log-density
  target += normal_lpdf(y | theta, sigma); // log-likelihood
}

```

In this Stan program, we let `theta` be a transformation of `mu`, `eta`, and `tau`
instead of declaring `theta` in the `parameters` block, which allows the sampler will run more efficiently ([see detailed explanation](http://mc-stan.org/documentation/case-studies/divergences_and_bias.html)).
We can prepare the data (which typically is a named list) in R with: 

## Example Eight Schools - Run

```{r, eval=FALSE}
schools_dat <- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
```

And we can get a fit with the following R command. Note that the argument to `file = ` should point to where the file is on your file system unless you have put it in the working directory of R in which case the below will work. 

```{r, eval=FALSE}
fit <- stan(file = 'schools.stan', data = schools_dat)
```

```{r eval=FALSE, echo=FALSE}
saveRDS(fit, file = "files/fit.RDS")
saveRDS(fit2, file= "files/fit2.RDS")
```


```{r, echo=FALSE}
fit <- readRDS(file = "files/fit.RDS")
fit2 <- readRDS(file= "files/fit2.RDS")
```

The object `fit`, returned from function `stan` is an [S4 object](https://cran.r-project.org/package=rstan/vignettes/stanfit-objects.html) of class
`stanfit`. 

## Example Eight Schools - Evaluate

Methods such as `print`, `plot`, and `pairs` are associated with the
fitted result so we can use the following code to check out the results in `fit`. 
`print` provides a summary for the parameter of the model as well
as the log-posterior with name `lp__` (see the following example output).
For more methods and details of class `stanfit`, see the help of class `stanfit`. 
We can use the `extract` function on `stanfit` objects to 
obtain the samples. 

```{r, eval=FALSE}
print(fit)
plot(fit)

la <- extract(fit, permuted = TRUE) # return a list of arrays 
mu <- la$mu 

### return an array of three dimensions: iterations, chains, parameters 
a <- extract(fit, permuted = FALSE) 

### use S3 functions on stanfit objects
a2 <- as.array(fit)
m <- as.matrix(fit)
d <- as.data.frame(fit)
```

## Example Eight Schools - Plot Distributions

```{r}
pairs(fit, pars = c("mu", "tau", "lp__"))
```

# References
