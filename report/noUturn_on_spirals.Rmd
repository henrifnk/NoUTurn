---
title: "UseCase_LogisticRonSpirals"
author: "Henri Funk"
date: "2 12 2020"
output: html_document
---

# Examples

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
longrun = FALSE

devtools::load_all()
library(mlbench)
library(ggplot2)
library(gridExtra)
```

## 20-Dimensional Multivariate Normal

```{r, eval=longrun}
set.seed(1234L)
covariance <- clusterGeneration::genPositiveDefMat(
			dim = 250,
			covMethod = "eigen", 
			eigenvalue = runif(250, 0, 1000))

inverse_covar <- solve(covariance$Sigma)

posterior_density  <- function(x) mvtnorm::dmvnorm(x, sigma = covar$Sigma, log = TRUE)
gradient <- function(position) as.numeric(-(inverse_covar %*% as.numeric(position)))

hmc_sample10 <- hamiltonianMC(rnorm(250), stepsize = 0.1, leapfrogsteps = 10, iterations = 2e3, is_log = TRUE)
hmc_sample100 <- hamiltonianMC(rnorm(250), stepsize = 0.01, leapfrogsteps = 100, iterations = 2e3, is_log = TRUE)
debugonce(naive_nouturn_sampler)
naivenuts <- naive_nouturn_sampler(runif(250, -5, 5), stepsize = 2, iteration = 2e3, is_log = TRUE, covar = diag(covar$egvalues))
effnuts_sample <- sample_noUturn(position_init = runif(250), iteration = 20, is_log = TRUE, covar = diag(covar$egvalues))
debugonce(sample_noUturn)
```

```{r, eval=longrun}
idxmin <- which(covar$egvalues == min(covar$egvalues))
idxmax <-  which(covar$egvalues == max(covar$egvalues))



plot_l100_samples <- function(x, title) {
  x <- x[, c(30, 209)]
  
  indep_samples <- as.data.frame(mvtnorm::rmvnorm(2e3, sigma = covar$Sigma))
  dens <- apply(indep_samples, 1, posterior_density)
  
  ggplot(data = indep_samples, aes(x = V30, y = V209, z = dens)) +
    stat_density2d(geom = "raster",
  aes(fill = after_stat(density)),
  contour = FALSE)+
  scale_fill_viridis_c() +
    geom_point(aes(x = x$X30, y = x$X209), alpha = 0.1) +
    ggtitle(title)
}

hmc10_plot <- plot_l100_samples(hmc_sample10, "Hamiltonian Monte Calo, stepsize = 0.1")
hmc100_plot <- plot_l100_samples(hmc_sample100, "Hamiltonian Monte Calo, stepsize = 0.01")

naivenuts_plot <- plot_l100_samples(naivenuts, "Naive NUTS, stepsize = 0.01")
effnuts_plot <- plot_l100_samples(effnuts_sample, "Naive NUTS, stepsize = 0.01")

grid.arrange(hmc10_plot, hmc100_plot, naivenuts_plot, nrow = 1)
```


```{r, echo =FALSE, eval=longrun}
saveRDS(sample50norm, file = "files/sample50norm.rds")
```

```{r, echo = FALSE, eval =!longrun}
sample50norm <- readRDS("files/sample50norm.rds")
```

```{r traceplot}
ggplot(data = sample50norm, aes(x = seq_len(2000), y = X10)) +
  geom_line()
ggplot(data = sample50norm[-seq_len(1e3), ], aes(x = X10)) +
  geom_density()
```
```{r}
attrib_mnd <- attributes(sample50norm)
knitr::kable(table(attrib_mnd$tree_depth), col.names = c("tree_depth", "freq"))
attrib_mnd$dual_averaging$stepsize_weight[1000]
```

## Base Case linear regression

Generate data

```{r}
design <- cbind(1, sapply(1:2, function(x) runif(400)))
perfect_position <- c(1,2,3)
target <- design %*% perfect_position + rnorm(400)
```

```{r}
ggplot(as.data.frame(design[,-1]), aes(x =V1, y = V2, colour=target)) +
  geom_point() +
  scale_colour_gradient2()
```


```{r}
lm(target~., data = as.data.frame(design[,-1]))
```
```{r}
posterior_density = log_linear
gradient = partial_deriv_lin
```

```{r}
linear <- sample_noUturn(c(4, 4, 4), iteration = 5e3, design = design, target = target, is_log = TRUE, seed = 123L, target_accpentance = .65, max_tree_depth = 12L)
```
Trace Plot for parameters

```{r}
calculate_traceplots <- function(var, varname, iter = 5e3){
  ggplot(data = linear, aes_string(x = seq_len(iter), y = var)) +
    geom_line(color = "red") + labs(x = "iteration", y = varname) +
    annotate("rect", xmin = 0, xmax = 2500, ymin = 4, ymax = -1, alpha = .2)
}

args <- list(
  list(var = "X1", varname = "intercept"), list(var = "X2", varname = "slope2"),
  list(var = "X3", varname = "slope3"), list(var = "X4", varname = "slope4")
  )

traceplots <- lapply(args, function(x) do.call(calculate_traceplots, x))

do.call(grid.arrange, c(traceplots, nrow = 2))

```



## Classification

## SetUp Experiment

A data frame with 79 rows and 4 variables:
**NV** Neovasculation risk factor indicator (0=Absent, 1=Present)  
**PI** Pulsatility index of arteria uterina  
**EH** Endometrium height  

We standardize all of the features

```{r}
library(hmclearn)
data(Endometrial)


# data prep
Endometrial$PI2 <- with(Endometrial, (PI - mean(PI)) / sd(PI))
Endometrial$EH2 <- with(Endometrial, (EH - mean(EH)) / sd(EH))
Endometrial$NV2 <- Endometrial$NV - 0.5

X <- cbind(1, as.matrix(Endometrial[, which(colnames(Endometrial)
            %in% c("PI2", "EH2", "NV2"))]))
y <- Endometrial$HG

colnames(X) <- c("(Intercept)", "PI2", "EH2", "NV2")
```

## Define posterior and gradient function

```{r}
devtools::load_all()
posterior_density = bernoulli_pen
gradient = partial_deriv_benoulli
```

$$ \begin{aligned}
f(\mathbf{y} | \mathbf{X},\boldsymbol\beta) &= \prod_{i=1}^n p^{y_i} (1-p)^{1-y_i}, \\
&= \prod_{i=1}^{n} \left(\frac{1}{1+e^{-\mathbf{x}i^T\boldsymbol\beta}}\right)^{y_i} \left(\frac{e^{-\mathbf{x}_i^T\boldsymbol\beta}}{1+e^{-\mathbf{x}_i^T\boldsymbol\beta}}\right)^{1-y_i}, \\
\log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) &= \sum{i=1}^n -y_i\log(1+e^{-\mathbf{x}i^T\boldsymbol\beta}) + (1-y_i)(-\mathbf{x}_i\boldsymbol\beta - \log(1+e^{-\mathbf{x}_i^T\boldsymbol\beta})), \\
&= \sum{i=1}^n -\log(1+e^{-\mathbf{x}i\boldsymbol\beta}) - \mathbf{x}_i^T\boldsymbol\beta(1 - y_i), \\
&= \sum{i=1}^n \mathbf{x}_i^T\boldsymbol\beta(y_i - 1) - \log(1 + e^{-\mathbf{x}_i^T\boldsymbol\beta}), \\
&= \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}_n) - \mathbf{1}_n^T [ \log( 1 + e^{-\mathbf{X}\boldsymbol\beta})].
\end{aligned} $$

We set a multivariate normal prior for $\boldsymbol\beta$

$$
\begin{aligned} \boldsymbol\beta &\sim N(0, \sigma^2 \mathbf{I}), \end{aligned}
$$

with pdf, omitting constants

$$
\begin{aligned}
\pi(\boldsymbol\beta | \sigma^2) &= \frac{1}{\sqrt{\lvert 2\pi \sigma^2 \rvert }}e^{-\frac{1}{2}\boldsymbol\beta^T \boldsymbol\beta / \sigma^2}, \\
\log \pi(\boldsymbol\beta | \sigma^2) &= -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2}\boldsymbol\beta^T \boldsymbol\beta / \sigma^2, \\
&\propto -\frac{1}{2}\log \sigma^2 - \frac{\boldsymbol\beta^T\boldsymbol\beta}{2\sigma^2}. 
\end{aligned}
$$

Next, we derive the log posterior, omitting constants,

$$
\begin{aligned}
f(\boldsymbol\beta | \mathbf{X}, \mathbf{y}, \sigma^2) &\propto f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) \pi(\boldsymbol\beta | \sigma^2), \\
\log f(\boldsymbol\beta | \mathbf{X}, \mathbf{y}, \sigma^2) &  \propto \log f(\mathbf{y} | \mathbf{X}, \boldsymbol\beta) + \log \pi(\boldsymbol\beta|\sigma^2), \\
&\propto \sum_{i=1}^n \mathbf{x}i^T\boldsymbol\beta(y_i - 1) - \log(1 + e^{-\mathbf{x}_i^T\boldsymbol\beta}) - \frac{1}{2}\boldsymbol\beta^T\boldsymbol\beta / \sigma^2, \\
&\propto \boldsymbol\beta^T\mathbf{X}^T(\mathbf{y} - \mathbf{1}n) - \mathbf{1}_n^T[\log( 1 + e^{-\mathbf{X}\boldsymbol\beta})] - \frac{\boldsymbol\beta^T\boldsymbol\beta}{2\sigma^2}. 
\end{aligned} 
$$

Next, we need to derive the gradient of the log posterior for the leapfrog function

$$ 
\begin{aligned}
\nabla_{\boldsymbol\beta} \log f(\boldsymbol\beta, \mathbf{X}, \mathbf{y}, \sigma^2) &\propto \mathbf{X}^T \left ( \mathbf{y} - \mathbf{1}n+ \frac{e^{-\mathbf{X}\boldsymbol\beta}}{1 + e^{-\mathbf{X}\boldsymbol\beta}}\right) - \boldsymbol\beta / \sigma^2 
\end{aligned}
$$
Run `naive_nouturn_sampler` for 2000 iterations with a small stepsize of `0.45`
As our posterior is already logged we ca specify the `is_logged` argument as `TRUE`

We use a GAM with 2 position estimates indicating sinus of their true position here
to consider spirals nonlinear structure.

```{r, eval=longrun}
hmc <- hamiltonianMC(runif(4, -5, 5), iterations = 5e3,stepsize = 0.1, leapfrogsteps = 10, design =  X, target = y)
```

```{r eval=longrun, echo=FALSE}
saveRDS(object = hmc, file = "files/hmc.RDS")
```

```{r eval=!longrun, echo=FALSE}
hmc <- readRDS("files/hmc.RDS")
````

```{r, echo=FALSE}
calculate_traceplots <- function(var, varname, iter = 5e3){
  ggplot(data = hmc, aes_string(x = seq_len(iter), y = var)) +
    geom_line(color = "purple") + labs(x = "iteration", y = varname)
  }

args <- list(
  list(var = "X1", varname = "intercept"), list(var = "X2", varname = "slope2"),
  list(var = "X3", varname = "slope3"), list(var = "X4", varname = "slope4")
  )

traceplots <- lapply(args, function(x) do.call(calculate_traceplots, x))

do.call(grid.arrange, c(traceplots, nrow = 2))
```


```{r, eval=longrun}
initial_pos <- lapply(1:5, function(x) runif(9, -5, 5))
param_sample_binary <- lapply(initial_pos, sample_noUturn, iteration = 5e3, design = design, target = target, is_log = TRUE, seed = 123L, target_accpentance = .39, max_tree_depth = 12L)
```

```{r eval=longrun, echo=FALSE}
saveRDS(object = param_sample_binary, file = "files/param_sample_binary.RDS")
```

```{r eval=!longrun, echo=FALSE}
param_sample_binary <- readRDS("files/param_sample_binary.RDS")
```

## Check for convergence

Acceptance rate

Trace Plot for parameters

```{r}
calculate_traceplots <- function(var, varname, iter = 5e3){
  ggplot(data = param_sample_binary[[1]], aes_string(x = seq_len(iter), y = var)) +
    geom_line(color = "red") + labs(x = "iteration", y = varname) +
    geom_line(data =param_sample_binary[[2]],
            aes_string(x = seq_len(iter), y = var), color = "blue") +
    geom_line(data =param_sample_binary[[3]],
            aes_string(x = seq_len(iter), y = var), color = "green") +
    annotate("rect", xmin = 0, xmax = 2500, ymin = -5, ymax = 5, alpha = .2)
}

args <- list(
  list(var = "X1", varname = "intercept"), list(var = "X2", varname = "slope2"),
  list(var = "X3", varname = "slope3"), list(var = "X4", varname = "slope4")
  )

traceplots <- lapply(args, function(x) do.call(calculate_traceplots, x))

do.call(grid.arrange, c(traceplots, nrow = 2))

```

Slopes seem to converge after 1000 to 1250 iterations, while intercept is already quite stable from the beginning.

\rightarrow Leave out first 1250 iterations as burn in and calculate position estimates means and variance

```{r}
param_distrib <- lapply(param_sample_binary, function(x){
  x[-seq_len(2500),]
})

calc_density_plots <- function(var, varname) {
  ggplot() +
    geom_density(data = param_distrib[[1]], aes_string(x = var), color = "red") + 
    geom_density(data = param_distrib[[2]], aes_string(x = var), color = "blue") + 
    geom_density(data = param_distrib[[3]], aes_string(x = var), color = "green") +
    geom_density(data = param_distrib[[4]], aes_string(x = var), color = "purple") +
    ggtitle(paste("density", varname))
}

density <- lapply(args, function(x)calc_density_plots(x$var, x$varname))
do.call(grid.arrange, c(density, nrow = 2))
```
GLM Comparison

```{r}
f <- glmnet::glmnet(X, y, family = "binomial", lambda = 1/200)


f$a0
f$beta
```

## Mean Estimation

```{r}
mean_estimate <- lapply(param_distrib, colMeans)

calc <- lapply(mean_estimate, function(x) {
  1- 1/(1+ exp(X %*% x))
})

lapply(calc, function(x){
  sum(y - x) / nrow(x)
})
```
## Attributes

```{r}
attribs <- lapply(param_sample_binary, attributes)
lapply(attribs, function(x) knitr::kable(table(x$tree_depth)))
lapply(attribs, function(x) x$dual_averaging$stepsize_weight[2500])
```


# Regression

Data Set Information:

The data was retrieved from a set of 53500 CT images from 74 different
patients (43 male, 31 female).

Each CT slice is described by two histograms in polar space.
The first histogram describes the location of bone structures in the image,
the second the location of air inclusions inside of the body.
Both histograms are concatenated to form the final feature vector.
Bins that are outside of the image are marked with the value -0.25.

The class variable (relative location of an image on the axial axis) was
constructed by manually annotating up to 10 different distinct landmarks in
each CT Volume with known location. The location of slices in between
landmarks was interpolated.


Attribute Information:

1. patientId: Each ID identifies a different patient
2. - 241.: Histogram describing bone structures
242. - 385.: Histogram describing air inclusions
386. reference: Relative location of the image on the axial axis (class
value). Values are in the range [0; 180] where 0 denotes
the top of the head and 180 the soles of the feet.

```{r, eval=longrun}
tmp <- tempfile()
download.file(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/00206/slice_localization_data.zip",
  tmp)
ct_slice <- read.csv(unz(tmp, filename = "slice_localization_data.csv"), as.is = TRUE)
```

```{r, eval=longrun}
id <- sample(seq_len(53500), 1e4)
trainCT <- ct_slice[id, ]
target <- trainCT[, 386]
design <- as.matrix(cbind(1, trainCT[, -386]))
c(min(target), max(target)) # -> map on real numbers, maybe gamma distrib...
```

```{r, eval=longrun}
devtools::load_all()
posterior_density = gamma_distr
gradient = partial_deriv_gamma
```


```{r, eval=longrun}
start <- Sys.time()
initial_pos <- lapply(1:3, function(x) c(runif(1), runif(386, 16, 18)))
sigmoid_parameter_distrib <- lapply(initial_pos, sample_noUturn, iteration = 10, design = design, target = target, is_log = TRUE, seed = 123L, target_accpentance = .65, max_tree_depth = 11L)
dif2000iter <- Sys.time() - start
dif2000iter
```

